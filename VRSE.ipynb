{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MinMint01/VRSE/blob/main/VRSE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kqpqlFSSLn33"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "count=0\n",
        "success=True\n",
        "video=cv2.VideoCapture('/content/drive/MyDrive/VIDEOS/Input.mp4')\n",
        "if not video.isOpened():\n",
        "    print(\"Error opening video file!\")\n",
        "while success:\n",
        "    success,image=video.read()\n",
        "    if success:\n",
        "        cv2.imwrite(\"/content/drive/MyDrive/VIDEOS/InputFrames/frame%d.jpg\" % count,image)\n",
        "        count+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOMxBfIblTf6",
        "outputId": "e40a97a3-c863-41df-ad16-d731d28a8b8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the number of videos in the database: \n",
            "4\n",
            "video 0 : \n",
            "\n",
            "video 1 : \n",
            "\n",
            "video 2 : \n",
            "\n",
            "video 3 : \n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Enter the number of videos in the database: \")\n",
        "n=int(input())\n",
        "\n",
        "def split(x):\n",
        "  count=0\n",
        "  success=True\n",
        "  video=cv2.VideoCapture('/content/drive/MyDrive/VIDEOS/DataBase/video%d.mp4' % x)\n",
        "  if not video.isOpened():\n",
        "    print(\"Error opening database video file : video%d!\" % x)\n",
        "  print(\"video\",x,\": \\n\")\n",
        "  while success:\n",
        "    success,image=video.read()\n",
        "    if success:\n",
        "      cv2.imwrite(\"/content/drive/MyDrive/VIDEOS/Temp%d/frame%d.jpg\" % (x,count),image)\n",
        "      count+=1\n",
        "    else:\n",
        "      break\n",
        "\n",
        "for i in range(n):\n",
        "  split(i)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2S6fbhW9qYA5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00b22742-27c0-41ce-db2f-40fb577fea70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine similarity between the two images: 0.9985549\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "resnet18 = models.resnet18(pretrained=True)\n",
        "\n",
        "# Remove the final layer (the classification layer) from the network\n",
        "feature_extractor = torch.nn.Sequential(*list(resnet18.children())[:-1])\n",
        "\n",
        "# Define a function to extract features from an image using the ResNet-18 feature extractor\n",
        "def extract_features(image_path, feature_extractor):\n",
        "    # Load the image and apply the necessary transformations\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    transform = transforms.Compose([transforms.Resize(256),\n",
        "                                    transforms.CenterCrop(224),\n",
        "                                    transforms.ToTensor(),\n",
        "                                    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                                         std=[0.229, 0.224, 0.225])])\n",
        "    image = transform(image).unsqueeze(0)\n",
        "\n",
        "    # Extract features using the feature extractor\n",
        "    with torch.no_grad():\n",
        "        features = feature_extractor(image).squeeze()\n",
        "\n",
        "    return features.numpy()\n",
        "\n",
        "# Define a function to compare two images using cosine similarity\n",
        "def compare_images(image1_path, image2_path, feature_extractor):\n",
        "    # Extract features from both images\n",
        "    features1 = extract_features(image1_path, feature_extractor)\n",
        "    features2 = extract_features(image2_path, feature_extractor)\n",
        "\n",
        "    # Calculate cosine similarity between the two feature vectors\n",
        "    similarity = np.dot(features1, features2) / (np.linalg.norm(features1) * np.linalg.norm(features2))\n",
        "\n",
        "    return similarity\n",
        "\n",
        "# Example usage: compare two images using ResNet-18 features\n",
        "image1_path = '/content/drive/MyDrive/VIDEOS/InputFrames/frame101.jpg'\n",
        "image2_path = '/content/drive/MyDrive/VIDEOS/Temp0/frame833.jpg'\n",
        "similarity = compare_images(image1_path, image2_path, feature_extractor)\n",
        "print(\"Cosine similarity between the two images:\", similarity)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "import xlwt\n",
        "from xlwt import Workbook\n",
        "wb = Workbook()\n",
        "sheet1 = wb.add_sheet('CSim')\n",
        "\n",
        "resnet18 = models.resnet18(pretrained=True)\n",
        "\n",
        "# Remove the final layer (the classification layer) from the network\n",
        "feature_extractor = torch.nn.Sequential(*list(resnet18.children())[:-1])\n",
        "\n",
        "# Define a function to extract features from an image using the ResNet-18 feature extractor\n",
        "def extract_features(image_path, feature_extractor):\n",
        "    # Load the image and apply the necessary transformations\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    transform = transforms.Compose([transforms.Resize(256),\n",
        "                                    transforms.CenterCrop(224),\n",
        "                                    transforms.ToTensor(),\n",
        "                                    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                                         std=[0.229, 0.224, 0.225])])\n",
        "    image = transform(image).unsqueeze(0)\n",
        "\n",
        "    # Extract features using the feature extractor\n",
        "    with torch.no_grad():\n",
        "        features = feature_extractor(image).squeeze()\n",
        "\n",
        "    return features.numpy()\n",
        "\n",
        "# Define a function to compare two images using cosine similarity\n",
        "def compare_images(image1_path, image2_path, feature_extractor):\n",
        "    # Extract features from both images\n",
        "    features1 = extract_features(image1_path, feature_extractor)\n",
        "    features2 = extract_features(image2_path, feature_extractor)\n",
        "\n",
        "    # Calculate cosine similarity between the two feature vectors\n",
        "    similarity = np.dot(features1, features2) / (np.linalg.norm(features1) * np.linalg.norm(features2))\n",
        "\n",
        "    return similarity\n",
        "\n",
        "# Example usage: compare two images using ResNet-18 features\n",
        "ndb=int(input(\"Enter the number of frames in video 0: \"))\n",
        "\n",
        "image1_path = '/content/drive/MyDrive/VIDEOS/InputFrames/frame101.jpg'\n",
        "\n",
        "for i in range(n):\n",
        "  print(i)\n",
        "  image2_path='/content/drive/MyDrive/VIDEOS/Temp0/frame%d.jpg' % i\n",
        "  similarity = compare_images(image1_path, image2_path, feature_extractor)\n",
        "  sheet1.write(i,0,similarity)\n",
        "  print(\"Cosine similarity between the input image and frame%d in video 0:\" % i, similarity)\n",
        "wb.save('/content/drive/MyDrive/VIDEOS/Cosine Similarity.xls')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q460pciy34fG",
        "outputId": "f8e06a01-1b77-4fd4-f5d6-7579eb37cef8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the number of frames in video 0: 1572\n",
            "0\n",
            "Cosine similarity between the input image and frame0 in video 0: 0.99809754\n",
            "1\n",
            "Cosine similarity between the input image and frame1 in video 0: 0.9980881\n",
            "2\n",
            "Cosine similarity between the input image and frame2 in video 0: 0.99804133\n",
            "3\n",
            "Cosine similarity between the input image and frame3 in video 0: 0.99790317\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xlwt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TEqHsve_9PLO",
        "outputId": "a6490d94-a757-4fb7-9d6a-1d3ee0360aeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting xlwt\n",
            "  Downloading xlwt-1.3.0-py2.py3-none-any.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.0/100.0 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xlwt\n",
            "Successfully installed xlwt-1.3.0\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1lGvMzyr2bRMq4wieVnG_wpacUcNwv73n",
      "authorship_tag": "ABX9TyM9f8zGXI+d6pnSNV1spLlu",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}